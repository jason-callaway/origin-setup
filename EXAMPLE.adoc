= OpenShift Service on AWS EC2
Mark Lamourine

{toc}
== Introduction

This document describes a fairly complete deployment of the OpenShift
Origin service on Amazon Web Services (AWS) EC2.  It is intended to
illustrate the concepts and components involved in a real
deployment. It is not meant as a "cut-n-paste" installation
document. Any real deployment will require a proper design based on
the requirements and constraints of the intended operations and users.

== Users

OpenShift is a service to help offer services.  This means that the
term _user_ has different meanings to different people.  To avoid
confusion this document will use these terms to refer to different
users based on the type of interaction they will have with the
service.

implementer::
  This is a person who is creating the OpenShift Origin service. Her
  "users" are the operators and app developers.
operator::
  This is a person running the OpenShift service. This person monitors
  and manages the OpenShift service for the developers.
developer::
  Or _app developer_.  App developers create custom applications which
  run in OpenShift.  These are the services which are available to
  _users_.
user::
  The application end user. He doesn't actually come into this process much.

== Components

There are three public (visible) parts to an OpenShift Origin service:

.OpenShift Public Components
console::
  This is the public face of the OpenShift service. This is a web
  interface which allows developers to manage and monitor their application
  instances.
broker::
  This is the control center of the service.  When developers interact with
  their services either by the console or with the command line tool,
  the broker accepts and executes the requests.
nodes::
  The nodes are where the applications live.  Developer code
  management and end-user application work happen here. There will
  generally be more than one node attached to an OpenShift service.

In addition to the public face of OpenShift, there are also a set of
_back-end services_ which provide needed functions such as
authentication, messaging, data mangement and publication.

.OpenShift Back-End Components
datastore::
  This is the brain and memory of the service.  Currently implimented
  using MongoDB and MongOID.
message broker::
  These hosts pass control information between the OpenShift broker
  and nodes.  Be careful of the overloading of the term _broker_. They
  are two different things.
authentication::
  This is a service which provides user identification and access
  management. It may be as simple as an Apache Basic Auth +.htpasswd+
  file.  It may also be a full fledged LDAP, or Kerberos service (such
  as IPA or Active Directory).
publication::
  aka DNS. This is the only truely external component. You can't
  publish applications in private, it makes no sense.  This example
  will use AWS Route53 dynamic DNS, but it could be any commercial DNS
  service or an business internal service which accepts DNS TSIG or
  GSS-TSIG updates. (Commerical services may require a custom plugin).

== Deployment

The design here is idealized. While complete, it is also
minimal. However, it is also designed to allow growth.

The entire deployment will reside on EC2 _t1.micro_ instances, but
will be controlled, at least initially from a linux desktop.  When
running in a real service all of the hosts but the puppetmaster and
message brokers would need to user larger instances tuned to actual
use levels.

The service hosts will be registered in the +infra.example.com+
domain. Applications will be registered in the +app.example.com+
domain as they are created.

In addition to the OpenShift broker and nodes, the OpenShift service
depends on several back-end services.  The DNS service will use the
AWS Route53 service. In a small installation such as this it is
possible to run the MongoDB data store and the ActiveMQ messaging on
the same host, but I'll resist the temptation to better illustrate the
distinctions between the services.

These are the host instances which will be created:

.Deployment Components
[options="header"]
|====================
| Name | Function
| puppetmaster | The anchor host, contains configs, controls remaining 
  deployment and configuration
|data1 | The (initial) MongoDB database host.
|msg1  | The (initial) ActiveMQ message broker host
|console | The web interface for developer interactions
|broker1 | The primary control host for service users
|node1 | The initial host which will contain user applications
|====================


== Toolbox

The service development and deployment will depend on a number of
external services and software packages.  At the bottom this means the
OS (Red Hat Enterprise Linux or Fedora Linux), and the AWS EC2 IaaS
(Infrastructure as a Service) system.  Additional tools are needed to
interact with the AWS services and with the running hosts after
they've been created.

.Development and Deployment Tools
* AWS EC2 (IaaS)
* AWS Route53 (DNS)
* AWS-SDK (AWS interaction, rubygem)
* Fedora Linux (AWS AMI)
* Red Hat Enterprise Linux (AWS AMI)
* Puppet (host configuration management)
* Thor (task programming, rubygem)


== AWS Setup

AWS is a commercial service of amazon.com. To use AWS you need to
register, establish a payment method.  Then you need to create a set
of access credentials so that you can interact with the AWS services
using the REST protocols (using the AWS-API rubygem).  You will also
need to generate an SSH key pair so that AWS can give you login access
to your host instances.

See the AWS documentation for https://aws.amazon.com/[registration]
and creating
https://portal.aws.amazon.com/gp/aws/securityCredentials[security
credentials].  Create one set of AWS access keys and one Amazon EC2
key pair.

=== AWS Access Key

The access key has two parts: _AccessKeyId_ and
_SecretAccessKey_. Generate an access key and make a note of both (you
can see them again whenever you want using the AWS console).

Remember, these are your *keys* so keep them secure, in files only you
can read or write. Don't email them or paste them into IM systems.

These two values are used to authenticate interactions using the
AWS-API.

=== Amazon EC2 Key Pair (SSH)

The EC2 key pair is really an SSH key pair. When you create an EC2
instance, the EC2 service places a copy of your public key in a user
account on the instance (_root_ for RHEL6, _ec2-user_ for Fedora
18+). You place the private key in the +.ssh+ subdirectory of your
home directory and use that key (id) to log into your instances.

Again, the part you download is *private*, treat it that way.  


== Origin Setup

This Git Repository contains a set of Thor task scripts and Puppet
module definitions which will be useful
